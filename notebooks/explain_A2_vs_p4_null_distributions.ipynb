{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": [
    "# Why A₂ and p₄ Have Different Null Distributions\n",
    "\n",
    "## The observation\n",
    "\n",
    "Under permutation testing (null model fitting), both A₂ and p₄ have Broderick reference values close to zero, but their null distributions look very different:\n",
    "- **A₂**: null distribution is narrow, peaking near 0 → actual error falls at ~70th percentile (not significant)\n",
    "- **p₄**: null distribution is wide, spread far from 0 → actual error falls at ~17th percentile (significant)\n",
    "\n",
    "This is puzzling because both parameters start near zero. Why does p₄ drift so much further than A₂ when fitting random (permuted) data?\n",
    "\n",
    "## The answer: two effects work together\n",
    "\n",
    "1. **Structural effect — inside vs. outside the Gaussian**: A₂ scales the prediction amplitude (outside the Gaussian); p₄ shifts the tuning peak (inside log₂(Pᵥ), inside the Gaussian exponent). This creates a gradient amplification factor for p₄ that A₂ lacks. Even without any normalization, p₄ drifts ~2.5× more than A₂.\n",
    "\n",
    "2. **L2 normalization amplifies this**: the loss function normalizes predictions by their L2 norm before comparing to data. A₂’s amplitude changes are divided out by the norm → the loss becomes insensitive to A₂ → its gradient collapses to near zero as it moves. p₄’s shape changes survive normalization → the gradient stays nonzero → p₄ keeps drifting. This amplifies the drift ratio from ~2.5× to **~25×**.\n",
    "\n",
    "This notebook walks through: (1) what a gradient is, (2) the analytical derivation of both effects, and (3) a numerical simulation confirming both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 11})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "## Step 1: What is a gradient?\n",
    "\n",
    "The **gradient** of the loss $L$ with respect to a parameter $\\theta$ answers: *if I nudge $\\theta$ slightly, how much does $L$ change?*\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\lim_{\\epsilon \\to 0} \\frac{L(\\theta + \\epsilon) - L(\\theta)}{\\epsilon}$$\n",
    "\n",
    "The optimizer uses the gradient to update each parameter every training step:\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "**Critical consequence**: if $\\partial L / \\partial \\theta \\approx 0$, the update is nearly zero regardless of learning rate. A parameter with a consistently tiny gradient stays near its initialization even after thousands of training steps.\n",
    "\n",
    "This is precisely why A₂ stays near zero under null fitting: its gradient is tiny, so the optimizer never moves it far.\n",
    "\n",
    "PyTorch computes gradients automatically via `.backward()`. The cell below confirms this with a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example: L = (x - 3)^2, minimum at x = 3, gradient = 2(x - 3)\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "loss = (x - 3) ** 2\n",
    "loss.backward()  # PyTorch computes dL/dx automatically\n",
    "\n",
    "x_vals = np.linspace(-1, 7, 200)\n",
    "L_vals = (x_vals - 3) ** 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5, 3.8))\n",
    "ax.plot(x_vals, L_vals, 'k-', lw=2, label=r'$L = (x - 3)^2$')\n",
    "ax.plot(0, 9, 'ro', ms=11, zorder=5, label='current position (x = 0)')\n",
    "ax.plot(3, 0, 'g*', ms=14, zorder=5, label='minimum (x = 3, gradient = 0)')\n",
    "ax.annotate('', xy=(1.4, 9), xytext=(0.05, 9),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2.5))\n",
    "ax.text(0.1, 10.2,\n",
    "        f'gradient = {x.grad.item():.0f}\\n'\n",
    "        r'$\\Rightarrow$ move $x$ right toward minimum',\n",
    "        color='red', fontsize=10)\n",
    "ax.set_xlabel('x'), ax.set_ylabel('Loss L')\n",
    "ax.set_title('Gradient = slope of the loss at the current parameter value')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-04",
   "metadata": {},
   "source": [
    "## Step 2: The model equations and why gradients differ\n",
    "\n",
    "### The 2D model\n",
    "\n",
    "The model predicts the BOLD response for each stimulus as:\n",
    "\n",
    "$$\\text{pred} = \\underbrace{A_v}_{\\text{amplitude}} \\cdot \\underbrace{\\exp\\!\\left(-\\frac{u^2}{2\\sigma^2}\\right)}_{G \\;=\\; \\text{Gaussian}}, \\qquad u = \\log_2(\\omega_l) + \\log_2(P_v)$$\n",
    "\n",
    "- $\\omega_l$ is the stimulus spatial frequency\n",
    "- $u$ measures how far the stimulus is from the voxel’s tuning peak in log₂-frequency: $u = 0$ at peak, $|u| > 0$ for off-peak stimuli\n",
    "- $G = \\exp(-u^2/2\\sigma^2)$ is always in $[0, 1]$\n",
    "\n",
    "The two modulators are:\n",
    "$$A_v = 1 + A_1\\cos(2\\theta_l) + \\mathbf{A_2}\\cos(4\\theta_l) + \\ldots \\qquad \\text{\\textbf{A₂ lives here — outside the Gaussian}}$$\n",
    "\n",
    "$$P_v = (\\text{slope}\\cdot r + \\text{intercept})\\bigl(1 + p_1\\cos(2\\theta_l) + \\ldots + \\mathbf{p_4}\\cos\\!\\bigl(4(\\theta_l-\\theta_v)\\bigr)\\bigr) \\qquad \\text{\\textbf{p₄ lives here — inside log₂(Pᵥ), inside the Gaussian}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Effect 1 — Structural: gradient amplification from being inside the Gaussian\n",
    "\n",
    "**Gradient for A₂** — A₂ lives in $A_v$, which multiplies $G$ from the outside:\n",
    "$$\\frac{\\partial \\,\\text{pred}}{\\partial A_2} = G \\cdot \\cos(4\\theta_l)$$\n",
    "Since $G \\in [0,1]$ always, this is **bounded by 1**. No amplification of any kind.\n",
    "\n",
    "**Gradient for p₄** — p₄ lives in $P_v$, which appears as $\\log_2(P_v)$ inside the Gaussian exponent. Differentiating through the chain $p_4 \\to P_v \\to \\log_2(P_v) \\to u \\to G \\to \\text{pred}$:\n",
    "$$\\frac{\\partial\\, \\text{pred}}{\\partial p_4} = A_v \\cdot G \\cdot \\underbrace{\\frac{-u}{\\sigma^2 \\ln 2}}_{\\text{amplification factor}} \\cdot \\cos\\!\\bigl(4(\\theta_l - \\theta_v)\\bigr)$$\n",
    "\n",
    "The extra factor $|u|/\\sigma^2$ comes from:\n",
    "1. Differentiating $\\exp(-u^2/2\\sigma^2)$ through $u$ → brings down $-u/\\sigma^2$\n",
    "2. Differentiating $\\log_2(P_v)$ through $P_v$ → brings down $1/(P_v \\ln 2)$, which simplifies to $1/\\ln 2$ after $\\partial P_v/\\partial p_4$ cancellation\n",
    "\n",
    "For off-peak stimuli (the majority), $|u| > 0$, amplifying p₄’s gradient beyond A₂’s. **Without L2 normalization, p₄ drifts ~2.5× more than A₂ under null fitting.**\n",
    "\n",
    "---\n",
    "\n",
    "### Effect 2 — L2 normalization: amplitude changes become invisible to the loss\n",
    "\n",
    "The actual loss normalizes predictions before comparing them to data:\n",
    "$$L = \\left\\|\\frac{\\text{pred}}{\\|\\text{pred}\\|} - \\frac{\\text{target}}{\\|\\text{target}\\|}\\right\\|^2$$\n",
    "\n",
    "**What this does to A₂**: A₂ changes $A_v \\approx 1 + A_2 \\cdot \\cos(4\\theta) + \\ldots$, which is essentially a scalar multiplier on all predictions (an amplitude change). The L2 norm divides this out. As a result:\n",
    "- Moving A₂ slightly from its initial value changes only the amplitude of predictions\n",
    "- The norm cancels this → the loss stops responding → gradient collapses toward 0 → A₂ freezes in place\n",
    "\n",
    "**What this does to p₄**: p₄ shifts which stimulus receives the peak response — a **shape** change (the relative magnitudes of predictions change). Shape changes are preserved through L2 normalization (they alter the pattern of normalized predictions). The loss remains sensitive to p₄ → gradient stays nonzero → p₄ keeps drifting to fit random patterns.\n",
    "\n",
    "**With L2 normalization, the drift ratio increases from ~2.5× to ~25×.**\n",
    "\n",
    "---\n",
    "\n",
    "### The same logic explains the full null distribution figure\n",
    "\n",
    "| Parameter group | Role | Expected null width |\n",
    "|---|---|---|\n",
    "| **A₁, A₂** | Amplitude modulators (outside Gaussian) | **Narrowest**: structural bound + killed by L2 norm |\n",
    "| **p₁–p₄** | Shape modulators via log₂(Pᵥ) | Moderate: structural $|u|/\\sigma^2$ factor + survives L2 norm |\n",
    "| **intercept, slope** | Baseline frequency (slope scaled by eccentricity $r$) | Wide: same as p-params but $r$ provides extra scaling |\n",
    "| **σ** | Bandwidth: gradient $\\propto u^2/\\sigma^3$ | **Widest**: largest amplification factor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Synthetic stimuli ──────────────────────────────────────────────────────\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "n_stim = 40\n",
    "\n",
    "theta_l = torch.tensor(np.random.uniform(0, 2*np.pi, n_stim), dtype=torch.float32)\n",
    "theta_v = torch.tensor(np.random.uniform(0, 2*np.pi, n_stim), dtype=torch.float32)\n",
    "r_v     = torch.tensor(np.random.uniform(1, 10,      n_stim), dtype=torch.float32)\n",
    "w_l     = torch.tensor(np.random.uniform(0.5, 8,     n_stim), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def make_params():\n",
    "    \"\"\"Initialize near Broderick parameter values (all p and A params near 0).\"\"\"\n",
    "    return {\n",
    "        'sigma':     torch.tensor(2.0,   requires_grad=True),\n",
    "        'slope':     torch.tensor(0.12,  requires_grad=True),\n",
    "        'intercept': torch.tensor(0.20,  requires_grad=True),\n",
    "        'p_1':       torch.tensor(0.05,  requires_grad=True),\n",
    "        'p_2':       torch.tensor(-0.03, requires_grad=True),\n",
    "        'p_3':       torch.tensor(0.04,  requires_grad=True),\n",
    "        'p_4':       torch.tensor(0.01,  requires_grad=True),\n",
    "        'A_1':       torch.tensor(0.04,  requires_grad=True),\n",
    "        'A_2':       torch.tensor(0.01,  requires_grad=True),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_pred(p):\n",
    "    \"\"\"Forward pass: 2D model prediction equations.\"\"\"\n",
    "    Av = torch.clamp(\n",
    "        1 + p['A_1'] * torch.cos(2 * theta_l)\n",
    "          + p['A_2'] * torch.cos(4 * theta_l),\n",
    "        min=1e-6)\n",
    "    Pv = torch.clamp(\n",
    "        (p['slope'] * r_v + p['intercept']) * (\n",
    "            1\n",
    "            + p['p_1'] * torch.cos(2 * theta_l)\n",
    "            + p['p_2'] * torch.cos(2 * theta_v)\n",
    "            + p['p_3'] * torch.cos(2 * (theta_l - theta_v))\n",
    "            + p['p_4'] * torch.cos(4 * (theta_l - theta_v))),\n",
    "        min=1e-6)\n",
    "    u = torch.log2(w_l) + torch.log2(Pv)\n",
    "    return Av * torch.exp(-u**2 / (2 * p['sigma']**2))\n",
    "\n",
    "\n",
    "def run_null_fitting(n_epochs=2000, use_l2_norm=True, seed=7):\n",
    "    \"\"\"Fit model to random (null) target data. Returns parameter trajectories.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    target    = torch.rand(n_stim)\n",
    "    params    = make_params()\n",
    "    optimizer = torch.optim.Adam(list(params.values()), lr=1e-3)\n",
    "    history   = {'A_2': [], 'p_4': []}\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = compute_pred(params)\n",
    "        if use_l2_norm:\n",
    "            pred_n   = pred   / torch.linalg.norm(pred)\n",
    "            target_n = target / torch.linalg.norm(target)\n",
    "            loss = torch.mean((pred_n - target_n) ** 2)\n",
    "        else:\n",
    "            loss = torch.mean((pred - target) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history['A_2'].append(params['A_2'].item())\n",
    "        history['p_4'].append(params['p_4'].item())\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Run both conditions (same random seed and target for fair comparison)\n",
    "hist_no_norm = run_null_fitting(use_l2_norm=False)\n",
    "hist_l2_norm = run_null_fitting(use_l2_norm=True)\n",
    "\n",
    "# ── Plot ────────────────────────────────────────────────────────────────────\n",
    "epochs = np.arange(2000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4.2))\n",
    "\n",
    "panel_info = [\n",
    "    (hist_no_norm, 'Without L2 normalization\\n(structural effect only)'),\n",
    "    (hist_l2_norm, 'With L2 normalization (actual loss)\\n(structural + amplitude-blindness)'),\n",
    "]\n",
    "\n",
    "for ax, (hist, title) in zip(axes, panel_info):\n",
    "    ax.plot(epochs, hist['A_2'], color='#e74c3c', lw=2,\n",
    "            label='A\\u2082  (amplitude modulator, outside Gaussian)')\n",
    "    ax.plot(epochs, hist['p_4'], color='#3498db', lw=2,\n",
    "            label='p\\u2084  (shape modulator, inside log\\u2082(P\\u1d65))')\n",
    "    ax.axhline(0.01, color='gray', ls='--', lw=1, alpha=0.7, label='initialization')\n",
    "\n",
    "    a2_final = hist['A_2'][-1]\n",
    "    p4_final = hist['p_4'][-1]\n",
    "    ratio    = abs(p4_final) / max(abs(a2_final), 1e-6)\n",
    "    ax.set_title(f'{title}\\nfinal A\\u2082={a2_final:.3f}, p\\u2084={p4_final:.3f}  '\n",
    "                 f'(drift ratio |p\\u2084|/|A\\u2082| \\u2248 {ratio:.0f}\\u00d7)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Parameter value')\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "\n",
    "plt.suptitle('Null fitting: A\\u2082 vs p\\u2084 trajectories when fitting random data',\n",
    "             fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-06",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Two effects, same direction\n",
    "\n",
    "| Effect | Mechanism | Contribution |\n",
    "|---|---|---|\n",
    "| **Structural** | p₄ is inside log₂(Pᵥ) inside the Gaussian exponent → gradient has extra $|u|/\\sigma^2$ factor. A₂ is outside → gradient bounded by $G \\leq 1$. | ~2.5× larger drift for p₄ |\n",
    "| **L2 normalization** | A₂ changes prediction amplitude → divided out by norm → loss becomes flat → gradient collapses → A₂ freezes. p₄ changes prediction shape → survives norm → gradient stays nonzero. | Amplifies total ratio to ~25× |\n",
    "\n",
    "### Why this explains the full null distribution figure\n",
    "\n",
    "The same two-effect logic predicts the ordering of all 9 parameters:\n",
    "\n",
    "- **A₁, A₂** (amplitude modulators, outside Gaussian): amplitude changes killed by L2 norm + bounded structural gradient → **narrowest** null distributions\n",
    "- **p₁–p₄** (shape modulators via log₂(Pᵥ)): $|u|/\\sigma^2$ amplification + shape changes survive L2 norm → **moderate** null distributions\n",
    "- **intercept, slope** (determine baseline Pᵥ for all stimuli; slope additionally scaled by eccentricity $r$): same as p-params but with larger effective gradient due to $r$ → **wide** null distributions\n",
    "- **σ** (bandwidth, in the denominator of the Gaussian exponent): gradient $\\propto u^2/\\sigma^3$, largest amplification factor of all → **widest** null distribution\n",
    "\n",
    "### Key takeaway\n",
    "\n",
    "The root cause is that **A₂ modulates amplitude while p₄ modulates shape**. This distinction arises from their structural positions in the model (outside vs. inside the Gaussian), and is then strongly amplified by the L2 normalization in the loss function, which is blind to amplitude but sensitive to shape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfp_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
